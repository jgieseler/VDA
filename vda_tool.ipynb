{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[SPEARHEAD VDA tool](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool receives a list of time references and outputs the corresponding plots derived from the VDA analysis based on user input.\n",
    "\n",
    "*The notebook cells are meant to be run serially. In cases where this is not applicable, the user is warned and instructed beforehand via the markdown cells.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [SPEARHEAD VDA tool](#toc1_)    \n",
    "  - [Imports & Setup](#toc1_1_)    \n",
    "  - [User Inputs](#toc1_2_)    \n",
    "    - [Input File](#toc1_2_1_)    \n",
    "    - [Particle Data](#toc1_2_2_)    \n",
    "    - [Energy Channels Grouping](#toc1_2_3_)    \n",
    "    - [Onset Determination Method](#toc1_2_4_)    \n",
    "    - [Onset Selection](#toc1_2_5_)    \n",
    "    - [Views/Displays](#toc1_2_6_)    \n",
    "  - [Run](#toc1_3_)    \n",
    "    - [Reference Times](#toc1_3_1_)    \n",
    "    - [Particle Data](#toc1_3_2_)    \n",
    "    - [Energies](#toc1_3_3_)    \n",
    "      - [Identify](#toc1_3_3_1_)    \n",
    "      - [Channel Grouping](#toc1_3_3_2_)    \n",
    "    - [Onset](#toc1_3_4_)    \n",
    "      - [Determination](#toc1_3_4_1_)    \n",
    "      - [Selection](#toc1_3_4_2_)    \n",
    "    - [VDA](#toc1_3_5_)    \n",
    "      - [Channels](#toc1_3_5_1_)    \n",
    "      - [Spacecraft](#toc1_3_5_2_)    \n",
    "      - [Plots](#toc1_3_5_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Imports & Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Imports & Setup\" section imports dependencies used throughout the tool and also indirectly defines constants and default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from solo_epd_loader import epd_load\n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import astropy.units as u\n",
    "from astropy.visualization import quantity_support\n",
    "from sunpy.coordinates import HeliocentricInertial\n",
    "import astrospice\n",
    "from pyonset import Onset, BootstrapWindow, OnsetStatsArray\n",
    "from ipywidgets import VBox, HBox\n",
    "import asyncio\n",
    "\n",
    "from vda_tool_configuration import *\n",
    "\n",
    "# omit Pandas' PerformanceWarning\n",
    "simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[User Inputs](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"User Inputs\" section parameterizes the notebook. The user should run the code cells one at a time and proceed to the next when the input fields are properly filled.\n",
    "\n",
    "In each cell there is a brief description for the asked inputs.\n",
    "\n",
    "*Note: In case prior input needs to be changed, all the input cells after the changed one shall be rerun to avoid inconsistencies.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Input File](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_input_type = widgets.Dropdown(options=[(\"Custom datetime range (1 event)\", 0),\n",
    "                                           (\"File with datetime ranges\", 1),\n",
    "                                           (\"File with reference datetimes\", 2)],\n",
    "                                  value=0,\n",
    "                                  description=\"How the events will be provided:\",\n",
    "                                  disabled=False,\n",
    "                                  style=WIDGETS_STYLE)\n",
    "wgt_input_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference datetimes and the rest of the parameters of this section define the time window for which to download particle data.\n",
    "\n",
    "- Reference times file: the path to the file which states the reference datetimes. This must be a .csv file with 2 columns. The first column is an `id` for the reference (event), while the second column is the reference datetime itself. The datetime format should be any format that is supported by the `pandas.to_pydatetime` function (i.e. 2024-12-31 00:00:00). The first line of the .csv file should be the headers, although the names of the headers don't matter to the rest of the tool.\n",
    "\n",
    "- Hours prior to the reference time: an integer indicating hours **before** the reference datetime. The derived datetime is the **start** of the time window for the particle data download.\n",
    "\n",
    "- Hours after the reference time: an integer indicating hours **after** the reference datetime. The derived datetime is the **end** of the time window for the particle data download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wgt_input_type.value == 0:\n",
    "    wgt_dt_start = widgets.Text(value=\"2021-05-22 19:45:00\",\n",
    "                                placeholder=\"format: YYYY-mm-dd HH:MM:SS\",\n",
    "                                description=\"Datetime range start:\",\n",
    "                                disabled=False,\n",
    "                                style=WIDGETS_STYLE,\n",
    "                                layout=WIDGETS_LAYOUT)\n",
    "    wgt_dt_end = widgets.Text(value=\"2021-05-23 02:45:00\",\n",
    "                                placeholder=\"format: YYYY-mm-dd HH:MM:SS\",\n",
    "                                description=\"Datetime range end:\",\n",
    "                                disabled=False,\n",
    "                                style=WIDGETS_STYLE,\n",
    "                                layout=WIDGETS_LAYOUT)\n",
    "    displaybox = HBox([wgt_dt_start, wgt_dt_end])\n",
    "elif wgt_input_type.value == 1:\n",
    "    wgt_dt_range_filepath = widgets.Text(value=\"examples/datetime_range_example.csv\",\n",
    "                                         placeholder=\"Path to .csv\",\n",
    "                                         description=\"Datetime range file:\",\n",
    "                                         disabled=False,\n",
    "                                         style=WIDGETS_STYLE,\n",
    "                                         layout=WIDGETS_LAYOUT)\n",
    "    displaybox = wgt_dt_range_filepath\n",
    "elif wgt_input_type.value == 2:\n",
    "    wgt_ref_times_filepath = widgets.Text(value=\"examples/reference_times_example.csv\",\n",
    "                                          placeholder=\"Path to .csv\",\n",
    "                                          description=\"Reference times file:\",\n",
    "                                          disabled=False,\n",
    "                                          style=WIDGETS_STYLE,\n",
    "                                          layout=WIDGETS_LAYOUT)\n",
    "    wgt_tw_prior = widgets.IntSlider(value=2,\n",
    "                                     min=0,\n",
    "                                     max=12,\n",
    "                                     step=1,\n",
    "                                     description=\"Hours prior to the reference time:\",\n",
    "                                     disabled=False,\n",
    "                                     style=WIDGETS_STYLE,\n",
    "                                     layout=WIDGETS_LAYOUT)\n",
    "    wgt_tw_after = widgets.IntSlider(value=5,\n",
    "                                     min=0,\n",
    "                                     max=12,\n",
    "                                     step=1,\n",
    "                                     description=\"Hours after the reference time:\",\n",
    "                                     disabled=False,\n",
    "                                     style=WIDGETS_STYLE,\n",
    "                                     layout=WIDGETS_LAYOUT)\n",
    "    displaybox = VBox([wgt_ref_times_filepath, wgt_tw_prior, wgt_tw_after])\n",
    "\n",
    "displaybox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Particle Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an option to load arleady downloaded data. In case the user has a proper .pkl file (outputted by this notebook) and wants to load the data from it, the \"Load data\" checkbox should be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_load_data = widgets.Checkbox(value=False,\n",
    "                                 description=\"Load data\",\n",
    "                                 disabled=False,\n",
    "                                 indent=True)\n",
    "wgt_load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case of \"Load data\" the user should provide the path to the .pkl file\n",
    "\n",
    "- In case of a run with new data: the user should state if he wants the data to be saved by checking the \"Save data\" checkbox and providing a path for the outputted file. The path should state a .pkl file to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wgt_load_data.value:\n",
    "    wgt_load_data_filepath = widgets.Text(value=\"\",\n",
    "                                          placeholder=\"Path to .pkl\",\n",
    "                                          description=\"File with saved DataFrame:\",\n",
    "                                          disabled=False,\n",
    "                                          style=WIDGETS_STYLE,\n",
    "                                          layout=WIDGETS_LAYOUT)\n",
    "    vbox = VBox([wgt_load_data_filepath])\n",
    "else:\n",
    "    wgt_save_data = widgets.Checkbox(value=False,\n",
    "                                     description=\"Save downloaded data\",\n",
    "                                     disabled=False,\n",
    "                                     indent=True)\n",
    "    wgt_save_data_filepath = widgets.Text(value=\"\",\n",
    "                                          placeholder=\"Path with .pkl extension\",\n",
    "                                          description=\"File to save data DataFrame:\",\n",
    "                                          disabled=False,\n",
    "                                          style=WIDGETS_STYLE,\n",
    "                                          layout=WIDGETS_LAYOUT)\n",
    "    vbox = VBox([wgt_save_data, wgt_save_data_filepath])\n",
    "    \n",
    "vbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user should check the sensor, species and viewings of the particle data to be downloaded.\n",
    "- Resample frequency: the frequency for the data to be resampled. This should be provided as an **offset alias**. [Reference](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for instructions and examples.\n",
    "\n",
    "**Important Note: In case of data load the checked items and frequency should match the loaded dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wgt_chk_sensors = [widgets.Checkbox(value=True,\n",
    "                                         description=sensor,\n",
    "                                         disabled=False,\n",
    "                                         indent=True)\n",
    "                        for sensor in AVAILABLE_SENSORS]\n",
    "grp_sensors = VBox(list_wgt_chk_sensors)\n",
    "\n",
    "list_wgt_chk_particles = [widgets.Checkbox(value=True,\n",
    "                                           description=particle,\n",
    "                                           disabled=False,\n",
    "                                           indent=True)\n",
    "                          for particle in AVAILABLE_PARTICLES]\n",
    "grp_particles = VBox(list_wgt_chk_particles)\n",
    "\n",
    "list_wgt_chk_viewings = [widgets.Checkbox(value=True,\n",
    "                                          description=viewing,\n",
    "                                          disabled=False,\n",
    "                                          indent=True)\n",
    "                         for viewing in AVAILABLE_VIEWINGS]\n",
    "grp_viewings = VBox(list_wgt_chk_viewings)\n",
    "\n",
    "wgt_resample_freq = widgets.Text(value=\"\",\n",
    "                                 placeholder=\"Valid offset aliases string (e.g. 5min, 5T, etc) - Leave blank for no resampling\",\n",
    "                                 description=\"Resample frequency:\",\n",
    "                                 disabled=False,\n",
    "                                 style=WIDGETS_STYLE,\n",
    "                                 layout=WIDGETS_LAYOUT)\n",
    "\n",
    "VBox([HBox([grp_sensors, grp_particles, grp_viewings]), wgt_resample_freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Energy Channels Grouping](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each series, multiply the values with the respective bin width and sum it all together. Then divide the result with the total bin width (sum of bin widths or max of last energy channel - min of first energy channel).\n",
    "\n",
    "- `<species>` group size: the number of `<species>` energy channels to be combined into a single energy channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_wgt_group_size = {}\n",
    "for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "    dict_wgt_group_size[particle] = widgets.IntSlider(value=2,\n",
    "                                                      min=1,\n",
    "                                                      max=6,\n",
    "                                                      step=1,\n",
    "                                                      description=f\"{particle.capitalize()} group size:\",\n",
    "                                                      disabled=False,\n",
    "                                                      style=WIDGETS_STYLE,\n",
    "                                                      layout=WIDGETS_LAYOUT)\n",
    "    \n",
    "VBox(list(dict_wgt_group_size.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[Onset Determination Method](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Onset determination method: the method to be utilized for the onset determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_onset_method = widgets.Dropdown(options=list(AVAILABLE_ONSET_METHODS.keys()),\n",
    "                                    value=list(AVAILABLE_ONSET_METHODS.keys())[0],\n",
    "                                    description=\"Onset determination method:\",\n",
    "                                    disabled=False,\n",
    "                                    style=WIDGETS_STYLE)\n",
    "wgt_onset_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method-specific parameters for the onset determination. For details refer to the relative method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_wgt_onset_params = {}\n",
    "for parameter, wgt_info in AVAILABLE_ONSET_METHODS[wgt_onset_method.value].items():\n",
    "    dict_wgt_onset_params[parameter] = wgt_info[\"widget\"](**wgt_info[\"widget_params\"])\n",
    "VBox(list(dict_wgt_onset_params.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_5_'></a>[Onset Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Onset determination method: choose how the onset times that will be used for the VDA plots should be chosen. Options:\n",
    "  - Use all: every determined onset time will be used\n",
    "  - Interactive: the execution of the notebook will be halted and the user will be prompted to decide whether to use the determined onset or not individually *(under development)*\n",
    "  - Custom list: all the determined onsets will be plotted and the execution of the notebook will be halted. The user should construct a list defining which viewing should be used per individual channel. The list items should follow the same order as the one of the displayed plots. *(under development)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_onset_selection = widgets.Dropdown(options=[(\"Use all\", 0),\n",
    "                                                (\"Interactive\", 1),\n",
    "                                                (\"Custom List\", 2)],\n",
    "                                       value=0,\n",
    "                                       description=\"Onset determination method:\",\n",
    "                                       disabled=False,\n",
    "                                       style=WIDGETS_STYLE)\n",
    "wgt_onset_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_6_'></a>[Views/Displays](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the produced DataFrames: this should be checked to display the intermediate DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_show_dfs = widgets.Checkbox(value=False,\n",
    "                                description=\"Display the produced DataFrames\",\n",
    "                                disabled=False,\n",
    "                                indent=True,\n",
    "                                style=WIDGETS_STYLE)\n",
    "\n",
    "VBox([wgt_show_dfs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Run](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rest of the cells can be run automatically**\n",
    "\n",
    "*In case the notebook purposefully halts, an error will be raised to the halted cell. The user should continue the run after the halted cell as instructed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Reference Times](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wgt_input_type.value == 0:\n",
    "    df_times = pd.DataFrame({START_TIME_COLNAME: [datetime.strptime(wgt_dt_start.value, \"%Y-%m-%d %H:%M:%S\")],\n",
    "                             END_TIME_COLNAME: [datetime.strptime(wgt_dt_end.value, \"%Y-%m-%d %H:%M:%S\")]},\n",
    "                             index=[1])\n",
    "elif wgt_input_type.value == 1:\n",
    "    df_times = pd.read_csv(wgt_dt_range_filepath.value,\n",
    "                           sep=\",\",\n",
    "                           header=0,\n",
    "                           names=[EVENT_INDEX_NAME, START_TIME_COLNAME, END_TIME_COLNAME],\n",
    "                           index_col=0)\n",
    "    df_times[START_TIME_COLNAME] = pd.to_datetime(df_times[START_TIME_COLNAME])\n",
    "    df_times[END_TIME_COLNAME] = pd.to_datetime(df_times[END_TIME_COLNAME])\n",
    "elif wgt_input_type.value == 2:\n",
    "    df_times = pd.read_csv(wgt_ref_times_filepath.value,\n",
    "                           sep=\",\",\n",
    "                           header=0,\n",
    "                           names=[EVENT_INDEX_NAME, REF_TIME_COLNAME],\n",
    "                           index_col=0)\n",
    "    df_times[REF_TIME_COLNAME] = pd.to_datetime(df_times[REF_TIME_COLNAME])\n",
    "    df_times[START_TIME_COLNAME] = df_times[REF_TIME_COLNAME].apply(lambda x: x - timedelta(hours=wgt_tw_prior.value))\n",
    "    df_times[END_TIME_COLNAME] = df_times[REF_TIME_COLNAME].apply(lambda x: x + timedelta(hours=wgt_tw_after.value))\n",
    "    df_times = df_times.drop(REF_TIME_COLNAME, axis=\"columns\")\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Particle Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(show_progress: bool = True) -> pd.DataFrame:\n",
    "    df_rows = []\n",
    "    keys = []\n",
    "    for index, row in df_times.iterrows():\n",
    "        if show_progress:\n",
    "            print(f\"Working on event {index}...\")\n",
    "        df_row = pd.DataFrame({})\n",
    "        keys.append(index)\n",
    "        for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "            for viewing in [w.description for w in list_wgt_chk_viewings if w.value]:\n",
    "                df_protons, df_electrons, _ = epd_load(sensor=sensor,\n",
    "                                                       level=\"l2\",\n",
    "                                                       startdate=row[START_TIME_COLNAME],\n",
    "                                                       enddate=row[END_TIME_COLNAME],\n",
    "                                                       viewing=viewing,\n",
    "                                                       path=DATA_PATH,\n",
    "                                                       autodownload=True)\n",
    "                if \"protons\" in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "                    if sensor == \"het\":\n",
    "                        flux_cols_name = \"H_Flux\"\n",
    "                    elif sensor == \"ept\":\n",
    "                        flux_cols_name = \"Ion_Flux\"\n",
    "                    df_protons = df_protons[[c for c in df_protons.columns if c[0] == flux_cols_name]]\n",
    "                    df_protons = df_protons[(df_protons.index >= row[START_TIME_COLNAME]) & (df_protons.index <= row[END_TIME_COLNAME])]\n",
    "                    if wgt_resample_freq.value is not None and wgt_resample_freq.value != \"\":\n",
    "                        df_protons = df_protons.resample(wgt_resample_freq.value, origin=\"start\").mean()\n",
    "                        df_protons.index = df_protons.index.floor(\"min\")\n",
    "                    df_protons = pd.concat([df_protons], keys=[(sensor, \"protons\", viewing)], axis=\"columns\")\n",
    "                    df_protons = df_protons.rename(lambda x: x.replace(flux_cols_name, PROTON_COLUMN_PREFIX), axis=\"columns\")\n",
    "                    df_row = pd.concat([df_row, df_protons], axis=\"columns\")\n",
    "                if \"electrons\" in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "                    if sensor == \"het\" or sensor == \"ept\":\n",
    "                        flux_cols_name = \"Electron_Flux\"\n",
    "                    df_electrons = df_electrons[[c for c in df_electrons.columns if c[0] == flux_cols_name]]\n",
    "                    df_electrons = df_electrons[(df_electrons.index >= row[START_TIME_COLNAME]) & (df_electrons.index <= row[END_TIME_COLNAME])]\n",
    "                    if wgt_resample_freq.value is not None and wgt_resample_freq.value != \"\":\n",
    "                        df_electrons = df_electrons.resample(wgt_resample_freq.value, origin=\"start\").mean()\n",
    "                        df_electrons.index = df_electrons.index.floor(\"min\")\n",
    "                    df_electrons = pd.concat([df_electrons], keys=[(sensor, \"electrons\", viewing)], axis=\"columns\")\n",
    "                    df_electrons = df_electrons.rename(lambda x: x.replace(flux_cols_name, ELECTRON_COLUMN_PREFIX), axis=\"columns\")\n",
    "                    df_row = pd.concat([df_row, df_electrons], axis=\"columns\")\n",
    "        df_rows.append(df_row)\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"Done\")\n",
    "    return pd.concat(df_rows, keys=keys, names=[EVENT_INDEX_NAME, \"Time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wgt_load_data.value:\n",
    "    df_data = pd.read_pickle(wgt_load_data_filepath.value)\n",
    "else:\n",
    "    df_data = download_data()\n",
    "    if wgt_save_data.value:\n",
    "        df_data.to_pickle(wgt_save_data_filepath.value)\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_3_'></a>[Energies](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_3_1_'></a>[Identify](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energies = pd.DataFrame({})\n",
    "df_sensors = []\n",
    "for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "    df_protons, df_electrons, energies = epd_load(sensor=sensor,\n",
    "                                                  level=\"l2\",\n",
    "                                                  startdate=df_times.iloc[0][START_TIME_COLNAME],\n",
    "                                                  enddate=df_times.iloc[0][END_TIME_COLNAME],\n",
    "                                                  viewing=\"sun\",\n",
    "                                                  path=DATA_PATH,\n",
    "                                                  autodownload=True)\n",
    "    if sensor == \"het\":\n",
    "        flux_cols_name = \"H_Flux\"\n",
    "        energy_bins_cols_name = \"H_Bins\"\n",
    "    elif sensor == \"ept\":\n",
    "        flux_cols_name = \"Ion_Flux\"\n",
    "        energy_bins_cols_name = \"Ion_Bins\"\n",
    "    df_protons = df_protons.rename(lambda x: x.replace(flux_cols_name, PROTON_COLUMN_PREFIX), axis=\"columns\")\n",
    "    df_electrons = df_electrons.rename(lambda x: x.replace(\"Electron_Flux\", ELECTRON_COLUMN_PREFIX), axis=\"columns\")\n",
    "\n",
    "    df_energies_protons = pd.DataFrame({\"Low Energy\": energies[f\"{energy_bins_cols_name}_Low_Energy\"], \"Bin Width\": energies[f\"{energy_bins_cols_name}_Width\"]},\n",
    "                                       index=df_protons[PROTON_COLUMN_PREFIX].columns)\n",
    "    df_energies_protons[\"High Energy\"] = df_energies_protons[\"Low Energy\"] + df_energies_protons[\"Bin Width\"]\n",
    "\n",
    "    df_energies_electrons = pd.DataFrame({\"Low Energy\": energies[\"Electron_Bins_Low_Energy\"], \"Bin Width\": energies[\"Electron_Bins_Width\"]},\n",
    "                                         index=df_electrons[ELECTRON_COLUMN_PREFIX].columns)\n",
    "    df_energies_electrons[\"High Energy\"] = df_energies_electrons[\"Low Energy\"] + df_energies_electrons[\"Bin Width\"]\n",
    "\n",
    "    df_sensors.append(pd.concat([df_energies_protons, df_energies_electrons]))\n",
    "\n",
    "df_energies = pd.concat(df_sensors, keys=[w.description for w in list_wgt_chk_sensors if w.value], names=[\"sensor\", \"channel\"])\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_3_2_'></a>[Channel Grouping](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_channels_de(df_to_group: pd.DataFrame, energy_bins_width: list[float], column_prefix: str = \"\", group_size: int = 2) -> pd.DataFrame:\n",
    "    # I = ΣI_n*ΔE_n / ΣΔE_n\n",
    "    channels = list(range(len(df_to_group.columns)))\n",
    "    grouped_channels = [channels[c:c+group_size] for c in range(0, len(channels), group_size)]\n",
    "    grouped_all = {}\n",
    "    for group in grouped_channels:\n",
    "        de = sum([energy_bins_width[group[i]] for i, _ in enumerate(group)])\n",
    "        grouped_series = df_to_group.iloc[:, group[0]]*energy_bins_width[group[0]]\n",
    "        for i, _ in enumerate(group[1:]):\n",
    "            grouped_series = grouped_series.add(df_to_group.iloc[:, group[i]]*energy_bins_width[group[i]], fill_value=0)\n",
    "        grouped_all[f\"{column_prefix}{'_' if column_prefix != '' else ''}{group[0]}-{column_prefix}{'_' if column_prefix != '' else ''}{group[-1]}\"] = grouped_series/de\n",
    "    df_grouped = pd.DataFrame(grouped_all)\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.DataFrame({})\n",
    "for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "    for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "        if particle == \"protons\":\n",
    "            particle_prefix = PROTON_COLUMN_PREFIX\n",
    "        elif particle == \"electrons\":\n",
    "            particle_prefix = ELECTRON_COLUMN_PREFIX\n",
    "        for viewing in [w.description for w in list_wgt_chk_viewings if w.value]:\n",
    "            df_temp = group_channels_de(df_data[sensor][particle][viewing][particle_prefix],\n",
    "                                        list(df_energies.loc[sensor][\"Bin Width\"]),\n",
    "                                        column_prefix=particle_prefix,\n",
    "                                        group_size={\n",
    "                                            particle: w.value\n",
    "                                            for particle, w in dict_wgt_group_size.items()\n",
    "                                        }[particle])\n",
    "            df_temp = pd.concat([df_temp], keys=[(sensor, particle, viewing, particle_prefix)], axis=\"columns\")\n",
    "            df_grouped = pd.concat([df_grouped, df_temp], axis=\"columns\")\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_4_'></a>[Onset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_4_1_'></a>[Determination](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _onset_detection_sigma(series: pd.Series,\n",
    "                           s: int = 3,\n",
    "                           n: int = 3,\n",
    "                           bg_start: int | datetime = 0,\n",
    "                           bg_end: int | datetime = 12) -> tuple:\n",
    "    \"\"\"Returns:\n",
    "    \n",
    "    1. Onset time or None if no event detected\n",
    "    2. Background start\n",
    "    3. Background end\n",
    "    4. Background Level\n",
    "    5. Threshold\n",
    "    \"\"\"\n",
    "    if type(bg_start) is int:\n",
    "        bg_start = series.index[bg_start]\n",
    "    if type(bg_end) is int:\n",
    "        bg_end = series.index[bg_end]\n",
    "    bg_level = (bg_series := series[bg_start:bg_end]).mean()\n",
    "    threshold = bg_level + s*bg_series.std()\n",
    "    onset_time = None\n",
    "\n",
    "    streak = 0\n",
    "    for index, value in series.items():\n",
    "        if value > threshold:\n",
    "            streak += 1\n",
    "            if onset_time is None:\n",
    "                onset_time = index\n",
    "        else:\n",
    "            streak = 0\n",
    "            onset_time = None\n",
    "\n",
    "        if streak >= n:\n",
    "            break\n",
    "    \n",
    "    if streak < n:\n",
    "        onset_time = None\n",
    "\n",
    "    return(onset_time, bg_start, bg_end, {\"bg_level\": bg_level, \"threshold\": threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _onset_detection_poisson_cusum_bootstrap(series: pd.Series,\n",
    "                                             sensor: str,\n",
    "                                             particle: str,\n",
    "                                             viewing: str,\n",
    "                                             channel: str,\n",
    "                                             bg_start: int | datetime = 0,\n",
    "                                             bg_end: int | datetime = 12,\n",
    "                                             bootstraps: int = 1000,\n",
    "                                             cusum_minutes: int = 60,\n",
    "                                             sample_size: float = 0.75,\n",
    "                                             limit_averaging: str = \"4 min\") -> tuple:\n",
    "    if type(bg_start) is int:\n",
    "        bg_start = series.index[bg_start]\n",
    "    if type(bg_end) is int:\n",
    "        bg_end = series.index[bg_end]\n",
    "    df = pd.DataFrame(series)\n",
    "    df.index.freq = wgt_resample_freq.value\n",
    "    protons = Onset(spacecraft=\"Solar Orbiter\",\n",
    "                    sensor=sensor.upper(),\n",
    "                    species=particle[:-1], # rest of the tool uses the plurar form. This function needs singular, so omit the final \"s\"\n",
    "                    viewing=viewing,\n",
    "                    data_level=\"l2\",\n",
    "                    data_path=\"\",\n",
    "                    start_date=\"\",\n",
    "                    end_date=\"\",\n",
    "                    data=df)\n",
    "    channels = channel.split(\"-\")\n",
    "    protons.set_custom_channel_energies(low_bounds=[df_energies.loc[sensor, channels[0]][\"Low Energy\"]],\n",
    "                                        high_bounds=[df_energies.loc[sensor, channels[1]][\"High Energy\"]],\n",
    "                                        unit=\"MeV\")\n",
    "    bg = BootstrapWindow(start=bg_start.strftime(\"%Y-%m-%d %H:%M\"), end=bg_end.strftime(\"%Y-%m-%d %H:%M\"), bootstraps=bootstraps)\n",
    "    rng = 101010101\n",
    "    protons.onset_statistics_per_channel(channels=channel,\n",
    "                                         background=bg,\n",
    "                                         cusum_minutes=cusum_minutes,\n",
    "                                         sample_size=sample_size,\n",
    "                                         viewing=protons.viewing,\n",
    "                                         limit_averaging=limit_averaging,\n",
    "                                         random_seed=rng,\n",
    "                                         print_output=False)\n",
    "    \n",
    "    return (protons.onset_statistics[channel][0], bg_start, bg_end, protons.onset_statistics[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_detection(series: pd.Series, method: str = \"sigma\", **kwargs) -> tuple:\n",
    "    if method == \"sigma\":\n",
    "        onset_results = _onset_detection_sigma(series,\n",
    "                                               kwargs[\"s\"],\n",
    "                                               kwargs[\"n\"],\n",
    "                                               kwargs[\"bg_start\"],\n",
    "                                               kwargs[\"bg_end\"])\n",
    "    elif method == \"poisson_cusum_bootstrap\":\n",
    "        onset_results = _onset_detection_poisson_cusum_bootstrap(series,\n",
    "                                                                 kwargs[\"sensor\"],\n",
    "                                                                 kwargs[\"particle\"],\n",
    "                                                                 kwargs[\"viewing\"],\n",
    "                                                                 kwargs[\"channel\"],\n",
    "                                                                 kwargs[\"bg_start\"],\n",
    "                                                                 kwargs[\"bg_end\"],\n",
    "                                                                 kwargs[\"bootstraps\"],\n",
    "                                                                 kwargs[\"cusum_minutes\"],\n",
    "                                                                 kwargs[\"sample_size\"],\n",
    "                                                                 kwargs[\"limit_averaging\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Method named \\\"{method}\\\" is not implented\")\n",
    "    return onset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_detection_df(df: pd.DataFrame, method: str = \"sigma\", **kwargs) -> dict:\n",
    "    df_onsets = pd.DataFrame({})\n",
    "    for index_event, df_event in df.groupby(level=0):\n",
    "        for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "            for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "                if particle == \"protons\":\n",
    "                    particle_prefix = PROTON_COLUMN_PREFIX\n",
    "                elif particle == \"electrons\":\n",
    "                    particle_prefix = ELECTRON_COLUMN_PREFIX\n",
    "                for viewing in [w.description for w in list_wgt_chk_viewings if w.value]:\n",
    "                    for column_name in (df_inner := df_event[sensor][particle][viewing][particle_prefix]).columns:\n",
    "                        kwargs[\"sensor\"] = sensor\n",
    "                        kwargs[\"particle\"] = particle\n",
    "                        kwargs[\"viewing\"] = viewing\n",
    "                        kwargs[\"channel\"] = column_name\n",
    "                        try:\n",
    "                            onset_time, bg_start, bg_stop, method_specific = onset_detection(df_inner[column_name].droplevel(0, axis=\"index\"),\n",
    "                                                                                                method,\n",
    "                                                                                                **kwargs)\n",
    "                            df_onsets = pd.concat([df_onsets, pd.DataFrame({\"Onset Time\": [onset_time],\n",
    "                                                                            \"Background Start\": [bg_start],\n",
    "                                                                            \"Background End\": [bg_stop],\n",
    "                                                                            \"Method Specific\": [method_specific]\n",
    "                                                                            },\n",
    "                                                                            index=[[index_event], [sensor], [particle], [viewing], [particle_prefix], [column_name]])])\n",
    "                        except Exception as e:\n",
    "                            print(index_event, type(e).__name__, kwargs)\n",
    "                            df_onsets = pd.concat([df_onsets, pd.DataFrame({\"Onset Time\": [pd.NaT],\n",
    "                                                                            \"Background Start\": [pd.NaT],\n",
    "                                                                            \"Background End\": [pd.NaT],\n",
    "                                                                            \"Method Specific\": [None]\n",
    "                                                                            },\n",
    "                                                                            index=[[index_event], [sensor], [particle], [viewing], [particle_prefix], [column_name]])])\n",
    "    df_onsets.index.names = [EVENT_INDEX_NAME, \"sensor\", \"particle\", \"viewing\", \"prefix\", \"channels\"]\n",
    "    return df_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onsets = onset_detection_df(df_grouped, wgt_onset_method.value, **{parameter: w.value\n",
    "                                                                      for parameter, w in dict_wgt_onset_params.items()})\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onsets_existing = df_onsets[~pd.isna(df_onsets[\"Onset Time\"])]\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_onsets_existing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_4_2_'></a>[Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_options = df_onsets_existing.reorder_levels([EVENT_INDEX_NAME, \"sensor\", \"particle\", \"prefix\", \"channels\", \"viewing\"])\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_onset(series: pd.Series,\n",
    "               onset_time: datetime,\n",
    "               bg_start_time: datetime,\n",
    "               bg_end_time: datetime,\n",
    "               title: str) -> None:\n",
    "    ax = series.fillna(0).plot(title=title,\n",
    "                               logy=True,\n",
    "                               label=\"Data\")\n",
    "    ax.set_ylim((ylim := ax.get_ylim())[0]*0.01, ylim[1]*100)\n",
    "    ax.axvline(onset_time, linestyle='--', label=\"Onset time\")\n",
    "    ylim_top = ax.get_ylim()[1]\n",
    "    ax.fill_between([bg_start_time, bg_end_time], 0, ylim_top, color=\"green\", alpha=0.25, label=\"BG sample\")\n",
    "    ax.set_ylim(top=ylim_top)\n",
    "    ax.legend()\n",
    "    plt.axes(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wgt_onset_selection.value == 0:\n",
    "    # Use all\n",
    "    chosen_onsets = None\n",
    "elif wgt_onset_selection.value == 1:\n",
    "    # Interactive\n",
    "    # from ipywidgets import Button\n",
    "    # widget_btns = [Button(description=viewing) for viewing in [w.description for w in list_wgt_chk_viewings if w.value]]\n",
    "    # widget_btns.append(Button(description=\"none\"))\n",
    "\n",
    "    # plt.ion()\n",
    "    # # fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    # fig, ax = plt.subplots()\n",
    "    # # line1, = ax.plot([pd.Timestamp('2020-07-21 05:15:00'), pd.Timestamp('2020-07-21 05:20:00')], [1, 10])\n",
    "    # line1, = ax.plot([], [])\n",
    "\n",
    "    # def wait_for_change(*widget_btns):\n",
    "    #     future = asyncio.Future()\n",
    "    #     def getvalue(change):\n",
    "    #         future.set_result(change.description)\n",
    "    #         for btn in widget_btns:\n",
    "    #             btn.on_click(getvalue, remove=True)\n",
    "    #         # we need to free up the binding to getvalue to avoid an IvalidState error\n",
    "    #         # buttons don't support unobserve\n",
    "    #         # so use `remove=True` \n",
    "    #     for btn in widget_btns:\n",
    "    #         btn.on_click(getvalue)\n",
    "    #     return future\n",
    "\n",
    "    # async def select():\n",
    "    #     df_selections = pd.DataFrame({})\n",
    "    #     for index_event, df_event in df_options.groupby(level=0):\n",
    "    #         for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "    #             for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "    #                 if particle == \"protons\":\n",
    "    #                     particle_prefix = PROTON_COLUMN_PREFIX\n",
    "    #                 elif particle == \"electrons\":\n",
    "    #                     particle_prefix = ELECTRON_COLUMN_PREFIX\n",
    "    #                 for channel, df_channel in df_event.loc[index_event, sensor, particle, particle_prefix].groupby(level=0):\n",
    "    #                     channel_low = (channels := channel.split(\"-\"))[0]\n",
    "    #                     channel_high = channels[1]\n",
    "    #                     # viewings = []\n",
    "    #                     # widget_btns = []\n",
    "    #                     for viewing, df_viewing in df_channel.groupby(level=1):\n",
    "    #                         plot_onset(df_grouped.loc[index_event][sensor, particle, viewing, particle_prefix, channel],\n",
    "    #                                 df_channel.loc[channel, viewing][\"Onset Time\"].to_pydatetime(),\n",
    "    #                                 df_channel.loc[channel, viewing][\"Background Start\"].to_pydatetime(),\n",
    "    #                                 df_channel.loc[channel, viewing][\"Background End\"].to_pydatetime(),\n",
    "    #                                 f\"Event {index_event}, {sensor}/{particle}, {df_energies.loc[sensor, channel_low]['Low Energy']:.2f}-{df_energies.loc[sensor, channel_high]['High Energy']:.2f} MeV, {viewing}\")\n",
    "    #                     #     series = df_grouped.loc[index_event][sensor, particle, viewing, particle_prefix, channel]\n",
    "    #                     #     onset_time = df_channel.loc[channel, viewing][\"Onset Time\"].to_pydatetime()\n",
    "    #                     #     bg_start_time = df_channel.loc[channel, viewing][\"Background Start\"].to_pydatetime()\n",
    "    #                     #     bg_end_time = df_channel.loc[channel, viewing][\"Background End\"].to_pydatetime()\n",
    "    #                     #     title = f\"Event {index_event}, {sensor}/{particle}, {df_energies.loc[sensor, channel_low]['Low Energy']:.2f}-{df_energies.loc[sensor, channel_high]['High Energy']:.2f} MeV, {viewing}\"\n",
    "    #                     #     # plt.ion()\n",
    "    #                     #     # fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    #                     #     # ax.plot(series.fillna(0))\n",
    "    #                     #     # ax = series.fillna(0).plot(title=title,\n",
    "    #                     #     #        logy=True,\n",
    "    #                     #     #     #    ylim=(1e-4, 1e4),\n",
    "    #                     #     #        label=\"Data\")\n",
    "    #                     #     series = series.fillna(0)\n",
    "    #                     #     line1.set_xdata(list(series.index))\n",
    "    #                     #     line1.set_ydata(list(series))\n",
    "    #                     #     # ax.set_title(title)\n",
    "    #                     #     # ax.set_ylim((ylim := ax.get_ylim())[0]*0.01, ylim[1]*100)\n",
    "    #                     #     # ax.axvline(onset_time, linestyle='--', label=\"Onset time\")\n",
    "    #                     #     # ylim_top = ax.get_ylim()[1]\n",
    "    #                     #     # ax.fill_between([bg_start_time, bg_end_time], 0, ylim_top, color=\"green\", alpha=0.25, label=\"BG sample\")\n",
    "    #                     #     # ax.set_ylim(top=ylim_top)\n",
    "    #                     #     # ax.legend()\n",
    "    #                     #     fig.canvas.draw()\n",
    "    #                     #     fig.canvas.flush_events()\n",
    "    #                     #     # plt.axes(ax)\n",
    "    #                     #     # plt.tight_layout()\n",
    "    #                     #     # plt.show()\n",
    "    #                     #     # viewings.append(viewing)\n",
    "    #                     #     # widget_btns.append(Button(description=viewing))\n",
    "    #                     # # viewings.append(\"none\")\n",
    "    #                     # # selection = None\n",
    "    #                     # # widget_btns.append(Button(description=\"none\"))\n",
    "    #                     # selection = await wait_for_change(*widget_btns)\n",
    "    #                     # print(selection)\n",
    "    #         #             break\n",
    "    #         #         break\n",
    "    #         #     break\n",
    "    #         # break\n",
    "\n",
    "    # asyncio.create_task(select())\n",
    "    # HBox(widget_btns)\n",
    "    pass\n",
    "elif wgt_onset_selection.value == 2:\n",
    "    # Custom list\n",
    "    df_selections = pd.DataFrame({})\n",
    "    for index_event, df_event in df_options.groupby(level=0):\n",
    "        for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "            for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "                if particle == \"protons\":\n",
    "                    particle_prefix = PROTON_COLUMN_PREFIX\n",
    "                elif particle == \"electrons\":\n",
    "                    particle_prefix = ELECTRON_COLUMN_PREFIX\n",
    "                for channel, df_channel in df_event.loc[index_event, sensor, particle, particle_prefix].groupby(level=0):\n",
    "                    channel_low = (channels := channel.split(\"-\"))[0]\n",
    "                    channel_high = channels[1]\n",
    "                    for viewing, df_viewing in df_channel.groupby(level=1):\n",
    "                        plot_onset(df_grouped.loc[index_event][sensor, particle, viewing, particle_prefix, channel],\n",
    "                                df_channel.loc[channel, viewing][\"Onset Time\"].to_pydatetime(),\n",
    "                                df_channel.loc[channel, viewing][\"Background Start\"].to_pydatetime(),\n",
    "                                df_channel.loc[channel, viewing][\"Background End\"].to_pydatetime(),\n",
    "                                f\"Event {index_event}, {sensor}/{particle}, {df_energies.loc[sensor, channel_low]['Low Energy']:.2f}-{df_energies.loc[sensor, channel_high]['High Energy']:.2f} MeV, {viewing}\")\n",
    "    \n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wgt_onset_selection.value == 2:\n",
    "    # chosen_onsets = [\"sun\", \"north\", None]\n",
    "    chosen_onsets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_5_'></a>[VDA](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_5_1_'></a>[Channels](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels_chars = pd.DataFrame({})\n",
    "for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "    for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "        if particle == \"protons\":\n",
    "            particle_prefix = PROTON_COLUMN_PREFIX\n",
    "        elif particle == \"electrons\":\n",
    "            particle_prefix = ELECTRON_COLUMN_PREFIX\n",
    "        for channel in list(df_grouped[sensor][particle][[w.description for w in list_wgt_chk_viewings if w.value][0]][particle_prefix].columns):\n",
    "            channels = channel.split(\"-\")\n",
    "            low_energy = df_energies.loc[sensor, channels[0]][\"Low Energy\"]\n",
    "            high_energy = df_energies.loc[sensor, channels[1]][\"High Energy\"]\n",
    "            geo_mean = sqrt(low_energy) * sqrt(high_energy)\n",
    "            inv_beta = 1/sqrt(1-(1/(1 + geo_mean/M_REST[particle]))**2)\n",
    "            df_channels_chars = pd.concat([df_channels_chars, pd.DataFrame({\"Geomagnetic Mean\": [geo_mean],\n",
    "                                                                            \"Inverse Beta\": [inv_beta]},\n",
    "                                                                            index=[[sensor], [particle], [channel]])])\n",
    "df_channels_chars.index.names = [\"sensor\", \"particle\", \"channel\"]\n",
    "\n",
    "if wgt_show_dfs.value:\n",
    "    display(df_channels_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_5_2_'></a>[Spacecraft](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = astrospice.registry.get_kernels('solar orbiter', 'predict')\n",
    "solo_kernel = kernels[0]\n",
    "coverage = solo_kernel.coverage('SOLAR ORBITER')\n",
    "print(coverage.iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "heliocentric = HeliocentricInertial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_5_3_'></a>[Plots](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_event, df_event in df_options.groupby(level=0):\n",
    "    vda_points = []\n",
    "    t_sun_to_observer = astrospice.generate_coords('SOLAR ORBITER', df_times.loc[index_event][START_TIME_COLNAME]).transform_to(heliocentric).distance.to(u.au)[0].value * AU_TO_M_RATIO / C\n",
    "    for sensor in [w.description for w in list_wgt_chk_sensors if w.value]:\n",
    "        try:\n",
    "            for particle in [w.description for w in list_wgt_chk_particles if w.value]:\n",
    "                if particle == \"protons\":\n",
    "                    particle_prefix = PROTON_COLUMN_PREFIX\n",
    "                elif particle == \"electrons\":\n",
    "                    particle_prefix = ELECTRON_COLUMN_PREFIX\n",
    "                for channel, df_channel in df_event.loc[index_event, sensor, particle, particle_prefix].groupby(level=0):\n",
    "                    channel_viewings = list(df_channel.loc[channel].index)\n",
    "                    if len(channel_viewings) == 1:\n",
    "                        if chosen_onsets is None or chosen_onsets.loc[index_event, sensor, particle, particle_prefix, channel][\"Viewing\"] == channel_viewings[0]:\n",
    "                            # if use every onset or the viewing matches the chosen\n",
    "                            vda_points.append((df_channels_chars.loc[sensor, particle, channel][\"Inverse Beta\"], df_channel.loc[channel, channel_viewings[0]][\"Onset Time\"].to_pydatetime().timestamp()))\n",
    "                    else:\n",
    "                        # Multiple viewings available\n",
    "                        if chosen_onsets is None:\n",
    "                            # Use every onset (default hierarchy of choice)\n",
    "                            for viewing in VIEWINGS_HIERARCHY:\n",
    "                                if viewing in channel_viewings:\n",
    "                                    vda_points.append((df_channels_chars.loc[sensor, particle, channel][\"Inverse Beta\"], df_channel.loc[channel, viewing][\"Onset Time\"].to_pydatetime().timestamp()))\n",
    "                                    break\n",
    "                        elif chosen_onsets.loc[index_event, sensor, particle, particle_prefix, channel][\"Viewing\"] in channel_viewings:\n",
    "                            # Chosen viewing exists\n",
    "                            vda_points.append((df_channels_chars.loc[sensor, particle, channel][\"Inverse Beta\"], df_channel.loc[channel, chosen_onsets.loc[index_event, sensor, particle, particle_prefix, channel][\"Viewing\"]][\"Onset Time\"].to_pydatetime().timestamp()))\n",
    "                        else:\n",
    "                            # Consider throwing warning (Chosen viewing not available)\n",
    "                            pass\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if len(vda_points) < 2:\n",
    "        # Not enough points for the linear regression\n",
    "        # Consider throughing warning\n",
    "        print(f\"Not enough onset points in event {index_event}.\")\n",
    "        continue\n",
    "    \n",
    "    vda_points = sorted(vda_points, key=lambda x: x[0])\n",
    "    inv_betas = np.array([p[0] for p in vda_points])\n",
    "    timestamps = np.array([p[1] for p in vda_points])\n",
    "\n",
    "    try:\n",
    "        p, V = np.polyfit(inv_betas, timestamps, 1, cov=True)\n",
    "        a = p[0]\n",
    "        b = p[1]\n",
    "        a_error = np.sqrt(V[0][0])\n",
    "        b_error = np.sqrt(V[1][1])\n",
    "    except ValueError:\n",
    "        # not enough points for cov matrix\n",
    "        print(f\"Not enough points for covariance matrix generation in event {index_event}\")\n",
    "        a, b = np.polyfit(inv_betas, timestamps, 1)\n",
    "        a_error = 0\n",
    "        b_error = 0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(inv_betas,\n",
    "               [datetime.fromtimestamp(t) for t in timestamps],\n",
    "               color=\"black\")\n",
    "    ax.plot(inv_betas,\n",
    "            [datetime.fromtimestamp(a*x+b) for x in inv_betas],\n",
    "            label=\"Linear Regression\",\n",
    "            color=\"blue\")\n",
    "    ax.fill_between(inv_betas,\n",
    "                    [datetime.fromtimestamp(a*x+b-2*b_error) for x in inv_betas],\n",
    "                    [datetime.fromtimestamp(a*x+b+2*b_error) for x in inv_betas],\n",
    "                    color=\"blue\",\n",
    "                    alpha=0.1)\n",
    "    plt.title(f\"Event {index_event}\")\n",
    "    plt.xlabel(\"Inverse Beta\")\n",
    "    plt.ylabel(\"Time\")\n",
    "    time_formatter = mdates.DateFormatter(\"%H:%M\")\n",
    "    ax.yaxis.set_major_formatter(time_formatter)\n",
    "    plt.plot([], [], alpha=0, label=f\"Extra Time = {str(timedelta(seconds=t_sun_to_observer)).split('.')[0]}\")\n",
    "    plt.plot([], [], alpha=0, label=f\"Release Time = {datetime.fromtimestamp(b + t_sun_to_observer).strftime('%Y-%m-%d %H:%M:%S')} +/- {str(timedelta(seconds=b_error)).split('.')[0]}\")\n",
    "    plt.plot([], [], alpha=0, label=f\"APL = {a / t_sun_to_observer:.2f} +/- {a_error / t_sun_to_observer:.2f}\")\n",
    "    plt.legend(bbox_to_anchor=(1, 0.6), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_vda_tool",
   "language": "python",
   "name": "venv_vda_tool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
